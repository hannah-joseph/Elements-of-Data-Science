Lab 8: Variance of Sample Means and Correlation
In this lab we will learn about the variance of sample means as well as ways to understand and quantify the association between two variables.

name = "Hannah Joseph"
name
'Hannah Joseph'
## import statements
# These lines load the tests. 
from gofer.ok import check
import numpy as np
from datascience import *
import pandas as pd
import matplotlib
%matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('ggplot')
# Fix for datascience plots
import collections as collections
import collections.abc as abc
collections.Iterable = abc.Iterable
1. How Faithful is Old Faithful?
(Note: clever title comes from here.)

Old Faithful is a geyser in Yellowstone National Park in the central United States. It's famous for erupting on a fairly regular schedule. You can see a video below.

# For the curious: this is how to display a YouTube video in a
# Jupyter notebook.  The argument to YouTubeVideo is the part
# of the URL (called a "query parameter") that identifies the
# video.  For example, the full URL for this video is:
#   https://www.youtube.com/watch?v=wE8NDuzt8eg
from IPython.display import YouTubeVideo
YouTubeVideo("wE8NDuzt8eg")

Some of Old Faithful's eruptions last longer than others. When it has a long eruption, there's generally a longer wait until the next eruption.

If you visit Yellowstone, you might want to predict when the next eruption will happen, so you can see the rest of the park and come to see the geyser when it erupts. To predict one variable from another, the first step is to understand the association between them.

The dataset has one row for each observed eruption. It includes the following columns:

duration: Eruption duration, in minutes
wait: Time between this eruption and the next, also in minutes
Run the next cell to load the dataset.

faithful = Table.read_table("faithful-new.csv")
faithful
duration	wait
3.6	79
1.8	54
3.333	74
2.283	62
4.533	85
2.883	55
4.7	88
3.6	85
1.95	51
4.35	85
... (262 rows omitted)

Question 1


Make a scatter plot of the data. It's conventional to put the column we will try to predict on the vertical axis and the other column on the horizontal axis. The predictor column is the independent or often labeled x variable.
predictor_col = 'duration'
predictor_col 

# This is the label as a string of the predictor column or independent x variable
'duration'
To plot a scatter plot need arrays froom columns of data
See example below to examine the Philadelphia Phillies winning percentage, PCT, as a function of year, YEAR.

# EXAMPLE DATA AND SCATTER PLOT
exampledata = Table().with_columns( \
     'YEAR', np.arange(2013,2023,1), \
     'PCT', make_array(.451,.451,.389,.438,.407,.494,.500,.467,.506,.537))
exampledata
YEAR	PCT
2013	0.451
2014	0.451
2015	0.389
2016	0.438
2017	0.407
2018	0.494
2019	0.5
2020	0.467
2021	0.506
2022	0.537
plt.scatter(exampledata.column('YEAR'),exampledata.column('PCT'),color="red") # need to use each column as an array to plot
plt.xlabel("Year")
plt.ylabel("Winning %")
#plt.show()
Text(0, 0.5, 'Winning %')

Regression Line
We can fit the observed trend using linear regression. We seek to understand the fundamental priciples behind this important manner of fitting data.

plt.scatter(exampledata.column('YEAR'),exampledata.column('PCT'),color="red") # need to use each column as an array to plot
plt.xlabel("Year")
plt.ylabel("Winning %")
# calculate equation for regression line
z = np.polyfit(exampledata.column('YEAR'),exampledata.column('PCT'), 1)
p = np.poly1d(z)
plt.annotate("Slope: "+str(round(p[0],2))+"  Intercept: "+str(round(p[1],4)),xy=(2014,0.52)) # Annotate with slope and intercept
# add regression line to plot
plt.plot(exampledata.column('YEAR'), p(exampledata.column('YEAR')),color="blue", linewidth=3, linestyle="--")
plt.show()

Now try to plot the faithful data
faithful.stats() # Helpful check on data, not neccessary in this case
statistic	duration	wait
min	1.6	43
max	5.1	96
median	4	76
sum	948.677	19284
# Now plot
plt.scatter(faithful.column(predictor_col),faithful.column('wait'))
plt.xlabel("duration")
plt.ylabel("wait")
plt.savefig("scatter.png") # Helpful way to save figure
plt.show()

Now try yourself, plot the reverse wait versus duration
plt.scatter(faithful.column('wait'), faithful.column(predictor_col))
plt.xlabel("wait")
plt.ylabel("duration")
plt.savefig("scatter_reverse.png")
plt.show()

check('tests/q1.py')
All tests passed!

Question 2

Look at the scatter plot. Are eruption duration and waiting time roughly linearly related? Is the relationship positive, as we claimed earlier? You may want to consult the textbook chapter 15 for the definition of "linearly related."

The eruption duration and waiting time are roughly lineraly related as the relationship is positive.

Standard units makes the analysis of the relationship between duration and wait more straightforward. First, we'll plot the data in standard units. Recall that, if nums is an array of numbers, then

(nums - np.mean(nums)) / np.std(nums)

is an array of those numbers in standard units.

Question 3


Compute the mean and standard deviation of the eruption durations and waiting times. **Then** create a table called `faithful_standard` containing the eruption durations and waiting times in standard units. (The columns should be named `"duration (standard units)"` and `"wait (standard units)"`.
duration_mean = np.mean(faithful.column('duration'))
duration_std = np.std(faithful.column('duration'))
wait_mean = np.mean(faithful.column('wait'))
wait_std = np.std(faithful.column('wait'))

faithful_standard = Table().with_columns(
    "duration (standard units)", (faithful.column('duration') - duration_mean) / duration_std,
    "wait (standard units)", (faithful.column('wait') - wait_mean) / wait_std)
faithful_standard
duration (standard units)	wait (standard units)
0.0984989	0.597123
-1.48146	-1.24518
-0.135861	0.228663
-1.0575	-0.655644
0.917443	1.03928
-0.530851	-1.17149
1.06403	1.26035
0.0984989	1.03928
-1.3498	-1.46626
0.756814	1.03928
... (262 rows omitted)

check('tests/q3.py')
All tests passed!

Question 4


Plot the data again, but this time in standard units.
# Review from above how plot a scatter plot
plt.scatter(faithful_standard.column('duration (standard units)'), faithful_standard.column('wait (standard units)')) 
plt.xlabel("duration (standard units)")
plt.ylabel("wait (standard units)")
plt.show()

You'll notice that this plot looks exactly the same as the last one! The data really are different, but the axes are scaled differently. (The method scatter scales the axes so the data fill up the available space.) So it's important to read the ticks on the axes.

Question 5

Among the following numbers, which would you guess is closest to the correlation between eruption duration and waiting time in this dataset?

-1
0
1
Assign your answer to closest_correlation.

closest_correlation = 1
check('tests/q5.py')
All tests passed!

Question 6


Compute the correlation `r`. *Hint:* Use `faithful_standard`. Section [15.1.2](https://inferentialthinking.com/chapters/15/1/Correlation.html#calculating-r) explains how to do this.
r = np.mean(faithful_standard.column('duration (standard units)') * faithful_standard.column('wait (standard units)'))
r
0.90081116832181318
check('tests/q6.py')
All tests passed!

The regression line
Recall that the correlation is the slope of the regression line when the data are put in standard units.

The next cell plots the regression line in standard units:

waiting time (standard units)
=
r
×
eruption duration (standard units)
.
Then, it plots the original data again, for comparison.

def plot_data_and_line(dataset, x, y, point_0, point_1):
    """Makes a scatter plot of the dataset, along with a line passing through two points. x and y are strings containing column labels"""
    xdata = dataset.column(x)
    ydata = dataset.column(y)
    plt.scatter(xdata, ydata, label="data")
    xs, ys = zip(point_0, point_1)
    plt.plot(xs, ys, label="regression line")
    plt.legend(bbox_to_anchor=(1.5,.8))

plot_data_and_line(faithful_standard, 
                   "duration (standard units)", 
                   "wait (standard units)", 
                   [-2, -2*r], 
                   [2, 2*r])

How would you take a point in standard units and convert it back to original units? We'd have to "stretch" its horizontal position by duration_std and its vertical position by wait_std.

That means the same thing would happen to the slope of the line.

Stretching a line horizontally makes it less steep, so we divide the slope by the stretching factor. Stretching a line vertically makes it more steep, so we multiply the slope by the stretching factor.

Question 7
What is the slope of the regression line in original units?

(If the "stretching" explanation is unintuitive, consult section 15.2.5 in the textbook.)

slope = r * (duration_std / wait_std)
slope
0.075627947951862715
We know that the regression line passes through the point (duration_mean, wait_mean). You might recall from high-school algebra that the equation for the line is therefore:

waiting time
−
wait_mean
=
slope
×
(
eruption duration
−
duration_mean
)
After rearranging that equation slightly, the intercept turns out to be:

intercept = slope*(-duration_mean) + wait_mean
intercept
70.633284945664954
check('tests/q7.py')
All tests passed!

2. Variability of the Sample Mean
By the Central Limit Theorem, the probability distribution of the mean of a large random sample is roughly normal. The bell curve is centered at the population mean. Some of the sample means are higher, and some lower, but the deviations from the population mean are roughly symmetric on either side, as we have seen repeatedly. Formally, probability theory shows that the sample mean is an unbiased estimate of the population mean.

In our simulations, we also noticed that the means of larger samples tend to be more tightly clustered around the population mean than means of smaller samples. In this section, we will quantify the variability of the sample mean and develop a relation between the variability and the sample size.

Let's take a look at the salaries of employees of the City of San Francisco in 2014. The mean salary reported by the city government was about $75463.92.

salaries = Table.read_table('sf_salaries_2014.csv').select("salary")
salaries
salary
471953
390112
339654
326717
326233
344187
311299
310161
335485
329391
... (38113 rows omitted)

salary_mean = np.mean(salaries.column('salary'))
salary_mean
75463.918140230307
salaries.hist('salary', bins=np.arange(0, 300000+10000*2, 10000))
plt.scatter(salary_mean, 0, marker='^', color='red', s=1000);
/opt/conda/lib/python3.10/site-packages/datascience/tables.py:5865: UserWarning: FixedFormatter should only be used together with FixedLocator
  axis.set_xticklabels(ticks, rotation='vertical')

Question 8


Clearly, the population does not follow a normal distribution. Keep that in mind as we progress through these exercises.
Let's take random samples and look at the probability distribution of the sample mean. As usual, we will use simulation to get an empirical approximation to this distribution.

We will define a function simulate_sample_mean to do this, because we are going to vary the sample size later. The arguments are the name of the table, the label of the column containing the variable, the sample size, and the number of simulations.

Complete the function simulate_sample_mean.

"""Empirical distribution of random sample means"""

def simulate_sample_mean(table, label, sample_size, repetitions):
    
    means = []

    for i in np.arange(repetitions):
        new_sample = table.sample(sample_size)
        new_sample_mean = np.mean(new_sample.column(label))
        means = np.append(means, new_sample_mean)

    sample_means = Table().with_column('Sample Means', means)
    
    # Display empirical histogram and print all relevant quantities – don't change this!
    fig, ax = plt.subplots()
    ax.hist(means, bins=20)
    plt.xlabel('Sample Means')
    plt.title('Sample Size ' + str(sample_size))
    
    textstr = '\n'.join((
    r'$\mathrm{Sample  Size}=%.2f$' % (sample_size, ),
    r'$\mathrm{Population  Mean}=%.2f$' % (np.mean(table.column(label)), ),
    r'$\mathrm{Average Of Sample Means}=%.2f$' % (np.mean(means), ), 
    r'$\mathrm{Population SD}=%.2f$' % (np.std(table.column(label)), ),
    r'$\mathrm{SD Of Sample Means}=%.2f$' % (np.std(means), )))

    # these are matplotlib.patch.Patch properties
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)

    # place a text box in upper left in axes coords
    ax.text(0.95, 0.75, textstr, transform=ax.transAxes, fontsize=14,
            verticalalignment='top', bbox=props)
    
    return means
check('tests/q8.py')
All tests passed!


Question 9


In the following cell, we will create a sample of size 100 from the salaries table and graph it using our new `simulate_sample_mean` function.
small_sample_size = simulate_sample_mean(salaries, 'salary', 100, 10000) 
plt.xlim(50000, 100000)
(50000.0, 100000.0)

In the following two cells, simulate the mean of a random sample of 400 salaries and 625 salaries, respectively. In each case, perform 10,000 repetitions of each of these processes. Don't worry about the plots.xlim line – it just makes sure that all of the plots have the same x-axis.

four_hundred_sample_mean= simulate_sample_mean(salaries, 'salary', 400, 10000) 
plt.xlim(50000, 100000)
(50000.0, 100000.0)

six_hundred_twenty_five_sample_mean= simulate_sample_mean(salaries, 'salary', 625, 10000) 
plt.xlim(50000, 100000)
(50000.0, 100000.0)

We can see the Central Limit Theorem in action – the histograms of the sample means are roughly normal, even though the histogram of the salaries themselves is far from normal.

We can also see that each of the three histograms of the sample means is centered very close to the population mean. In each case, the "average of sample means" is very close to the population mean. Both values are provided in the printout above each histogram. As expected, the sample mean is an unbiased estimate of the population mean.

Question 10

Below, we'll look at what happens when we take a fixed sample, then bootstrap from it with different numbers of resamples. How does the distribution of the resampled means change?

The distribution of the resampled means becomes has less variability as the number of resamples increases.

simulate_sample_mean(salaries, 'salary', 100, 500)
plt.xlim(50000, 100000)
(50000.0, 100000.0)

simulate_sample_mean(salaries, 'salary', 100, 1000)
plt.xlim(50000, 100000)
(50000.0, 100000.0)

simulate_sample_mean(salaries, 'salary', 100, 5000)
plt.xlim(50000, 100000)
(50000.0, 100000.0)

simulate_sample_mean(salaries, 'salary', 100, 10000)
plt.xlim(50000, 100000)
(50000.0, 100000.0)

Assign the variable bootstrap_sampled_SD to the integer corresponding to your answer to the following prediction question:

When I increase the number of bootstrap samples that I take, for a fixed sample size, the SD of my sample mean will...

Increase
Decrease
Stay about the same
Vary widly
bootstrap_sampled_SD = 3
check('tests/q10.py')
All tests passed!

What did you notice about the sample means of the four bootstrapped samples above? I noticed that the sample size remained the same and the standard deviation remains the same. However, the graphs show that the sample size means becomes slightly thinner and slightly varies depending the average sample size means.

Question 11


Next, let's think about how the relationships between population SD, sample SD, and SD of sample means change with varying sample size. Which of the following is true? Again, assign the variable `pop_vs_sample` to the integer corresponding to your answer. To gain some intuition, you can run the simulation cells below.
Sample SD gets smaller with increasing sample size, SD of sample means gets smaller with increasing sample size
Sample SD gets larger with increasing sample size, SD of sample means stays the same with increasing sample size
Sample SD becomes more consistent with population SD with increasing sample size, SD of sample means gets smaller with increasing sample size
Sample SD becomes more consistent with populatoin SD with increasing sample size, SD of smaple means stays the same with increasing sample size
pop_vs_sample = 3
check('tests/q11.py')
All tests passed!

Let's see what happens: First, we calculate the population SD so that we can compare the SD of each sample to the SD of the population.

pop_sd = np.std(salaries.column("salary"))
pop_sd
51697.034986465304
Let's then how a small sample behaves. Run the following cells multiple times to see how the SD of the sample changes from sample to sample. Adjust the bins as necessary.

sample_10 = salaries.sample(10)
sample_10.hist("salary")
print("Sample SD: ", np.std(sample_10.column("salary")))
means = simulate_sample_mean(sample_10, 'salary', 10, 1000)
/opt/conda/lib/python3.10/site-packages/datascience/tables.py:5865: UserWarning: FixedFormatter should only be used together with FixedLocator
  axis.set_xticklabels(ticks, rotation='vertical')
Sample SD:  56755.3898151


sample_200 = salaries.sample(200)
sample_200.hist("salary")
print("Sample SD: ", np.std(sample_200.column("salary")))
means = simulate_sample_mean(sample_200, 'salary', 200, 1000)
Sample SD:  48532.5979347


sample_1000 = salaries.sample(1000)
sample_1000.hist("salary")
print("Sample SD: ", np.std(sample_1000.column("salary")))
means = simulate_sample_mean(sample_1000, 'salary', 1000, 1000)
Sample SD:  48609.7475656


Let's illustrate this trend. Below, you will see how the average absolute error of SD from the population changes with sample size (N).

# Don't change this cell, just run it!
sample_n_errors = make_array()
for i in np.arange(10, 200, 10):
    sample_n_errors = np.append(sample_n_errors, np.average([abs(np.std(salaries.sample(i).column("salary"))-pop_sd)
                                                             for d in np.arange(100)]))
Table().with_columns("Average absolute error in SD", sample_n_errors, "N", np.arange(10, 200, 10)).plot("N", "Average absolute error in SD")

You should notice that the distribution of means gets spiker, and that the distribution of the sample increasingly looks like the distribution of the population as we get to larger sample sizes.

Is there a relationship between the sample size and absolute error in standard deviation? Identify this relationship – if you're having trouble, take a look at this section in our textbook about the variability of sample means.

There is a relationship between sample size and absolute error in standard deviation. As sample size increases, the absolute error in standard deviation decreases. This relationship is described based on the formula:

SD of all possible sample means (absolute error in SD) = population standard deviation / sqrt(sample size)

The formula shows that the standard error of the mean decreases as the sample size increases, because the denominator , which is the square root of the sample size, increases. Based on this, it means that larger sample sizes are associated with more accurate estimates of the population standard deviation.

Submission
You're finished with lab 8! In order to successfully submit your assignment, follow these steps...

IMPORTANT Before you do anything, Save and Checkpoint from the File menu. Please do this first before running the cell below,
run all the tests and verify that they all pass (the next cell has a shortcut for that),
Review the notebook one last time If you make any changes, please Save and Checkpoint again.
Download as an .html file and/or an .ipynb file and submit on Canvas!
# For your convenience, you can run this cell to run all the tests at once!
import glob
from gofer.ok import check
correct = 0
checks = [1,3,5,6,7,8,10,11]
total = len(checks)
for x in checks:
    print('Testing question {}: '.format(str(x)))
    g = check('tests/q{}.py'.format(str(x)))
    if g.grade == 1.0:
        print("Passed")
        correct += 1
    else:
        print('Failed')
        display(g)

print('Grade:  {}'.format(str(correct/total)))
Testing question 1: 
Passed
Testing question 3: 
Passed
Testing question 5: 
Passed
Testing question 6: 
Passed
Testing question 7: 
Passed
Testing question 8: 
Passed
Testing question 10: 
Passed
Testing question 11: 
Passed
Grade:  1.0
